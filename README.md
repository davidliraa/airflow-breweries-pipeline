# BEES Project â€“ Data Pipeline with Airflow and Medallion Architecture

## ğŸ“Œ Overview

This project implements a data pipeline based on the **Medallion architecture (Bronze, Silver, and Gold layers)**, using **Apache Airflow** for orchestration, **Docker** for containerization, and **Python/Pandas** for data processing.

The goal is to ingest public data from the **Open Brewery DB API**, perform transformations, and generate aggregations for future analysis.

---

## ğŸ› ï¸ Architecture and Data Flow

- **Bronze:**  
Raw data ingestion from the API. Files are saved in **JSON** format under `/data_lake/bronze/breweries`.

- **Silver:**  
Data cleaning and transformation. Processed data is exported in **partitioned Parquet** format by state to `/data_lake/silver/breweries`.

- **Gold:**  
Data aggregation (e.g., **brewery count by type and state**). Results are saved as CSV files in `/data_lake/gold/breweries`.

---

## ğŸ§° Technologies and Tools

- **Apache Airflow:** DAG orchestration (Bronze â†’ Silver â†’ Gold)
- **Docker / Docker Compose:** Environment containerization
- **Python 3.x + Pandas:** Data transformation and manipulation
- **PostgreSQL:** Airflow metadata backend

---

## ğŸ“‚ Folder Structure

```
.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ bronze_ingest_breweries.py
â”‚   â”œâ”€â”€ silver_transform_breweries.py
â”‚   â””â”€â”€ gold_aggregate_breweries.py
â”œâ”€â”€ data_lake/
â”‚   â”œâ”€â”€ bronze/breweries/
â”‚   â”œâ”€â”€ silver/breweries/
â”‚   â””â”€â”€ gold/breweries/
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## â–¶ï¸ How to Run

1. Configure the `.env` file with your local paths.
2. Start the containers:  
   ```bash
   docker-compose up --build
   ```
3. Access the Airflow UI:  
   [http://localhost:8080](http://localhost:8080)
4. Manually run the DAGs in the following order:
   - **bronze_ingest_breweries**
   - **silver_transform_breweries**
   - **gold_aggregate_breweries**
5. Check the output files in `/data_lake/gold/breweries`.

---

## âœ… Final Notes

- Ensure correct volume mapping for data and log persistence.
- All execution logs are available via the Airflow Web UI.
- The project was fully validated end-to-end in a local environment.

---

## ğŸ‘¤ Contact

**David Lira â€“ Data Engineer**  
[LinkedIn](https://www.linkedin.com/in/david-a-lira/)
